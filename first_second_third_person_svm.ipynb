{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "df = pd.read_csv('mbti_1.csv')\n",
    "\n",
    "# replace URLs\n",
    "\n",
    "# replace MBTI\n",
    "# https://stackoverflow.com/questions/16720541/python-string-replace-regular-expression/16720705\n",
    "mbti_pat = r\"ISFJ|ESFP|ISFP|ISTP|ENFP|ENFJ|INFJ|ESTP|ESFJ|ESTJ|ENTP|INFP|INTP|INTJ|ISTJ|ENTJ\"\n",
    "mbti_regex = re.compile(mbti_pat, re.IGNORECASE)\n",
    "MBTI_REP = '$MBTI$'\n",
    "\n",
    "# replace hashtags\n",
    "hashtag_pat = r\"(\\#[a-zA-Z0-9]+\\b)\"\n",
    "hashtag_regex = re.compile(hashtag_pat)\n",
    "HASHTAG_REP = '$HASHTAG$'\n",
    "\n",
    "# Replace links with $link$\n",
    "# https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url\n",
    "link_pat = r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
    "LINK_REP = '$LINK$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posts'] = df['posts'].apply(lambda x: re.sub(mbti_pat, MBTI_REP, x))\n",
    "df['posts'] = df['posts'].apply(lambda x: re.sub(hashtag_pat, HASHTAG_REP, x))\n",
    "df['posts'] = df['posts'].apply(lambda x: re.sub(link_pat, LINK_REP, x))\n",
    "\n",
    "\n",
    "df['posts'] = df['posts'].apply(lambda x: x.replace('|||', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IE'] = df['type'].apply(lambda x: 'I' if x[0] == 'I' else 'E')\n",
    "df['NS'] = df['type'].apply(lambda x: 'N' if x[1] == 'N' else 'S')\n",
    "df['FT'] = df['type'].apply(lambda x: 'F' if x[2] == 'F' else 'T')\n",
    "df['PJ'] = df['type'].apply(lambda x: 'P' if x[3] == 'P' else 'J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type', 'posts', 'IE', 'NS', 'FT', 'PJ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ie = df[['type', 'posts', 'IE']]\n",
    "df_ns = df[['type', 'posts', 'NS']]\n",
    "df_ft = df[['type', 'posts', 'FT']]\n",
    "df_pj = df[['type', 'posts', 'PJ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Double data values for underrepresented traits\n",
    "# df_ie = df_ie.append(df_ie[df_ie['IE'] == 'E'])\n",
    "# df_ns = df_ns.append(df_ns[df_ns['NS'] == 'S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pct = 0.6\n",
    "\n",
    "# indicates the location to split the data along\n",
    "# since dev/test are the same size\n",
    "test_split_position = 1.0 - (1.0 - train_pct) / 2\n",
    "test_split_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ie, dev_ie, test_ie = np.split(df_ie.sample(frac=1, random_state = 224), [int(train_pct*len(df_ie)), int(test_split_position*len(df_ie))])\n",
    "train_ns, dev_ns, test_ns = np.split(df_ns.sample(frac=1, random_state = 224), [int(train_pct*len(df_ns)), int(test_split_position*len(df_ns))])\n",
    "train_ft, dev_ft, test_ft = np.split(df_ft.sample(frac=1, random_state = 224), [int(train_pct*len(df_ft)), int(test_split_position*len(df_ft))])\n",
    "train_pj, dev_pj, test_pj = np.split(df_pj.sample(frac=1, random_state = 224), [int(train_pct*len(df_pj)), int(test_split_position*len(df_pj))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>IE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Well, I thought this would never happen but I...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I'm Type 9 and people in my family (who aren'...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'He mentioned extroversion and then you mentio...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8340</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I wish to change my name to War pigs and than...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Some just talk and want to be heard.  My 11th...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts IE\n",
       "3325  INTP  'Well, I thought this would never happen but I...  I\n",
       "43    INFP  'I'm Type 9 and people in my family (who aren'...  I\n",
       "902   INTP  'He mentioned extroversion and then you mentio...  I\n",
       "8340  ENTP  'I wish to change my name to War pigs and than...  E\n",
       "3392  INFJ  'Some just talk and want to be heard.  My 11th...  I"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_ie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5205, 95312)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# baseline for IE\n",
    "train_counts_ie = count_vect.fit_transform(list(train_ie['posts']))\n",
    "train_counts_ie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5800     965\n",
       "5750    1401\n",
       "3823    1083\n",
       "6183    1247\n",
       "5009     842\n",
       "8123    1244\n",
       "1027    1500\n",
       "7278    1245\n",
       "1334    1369\n",
       "370     1461\n",
       "3804      86\n",
       "678     1432\n",
       "116     1277\n",
       "3480    1379\n",
       "8109    1411\n",
       "4912    1643\n",
       "7482    1321\n",
       "5668    1249\n",
       "6486    1102\n",
       "8263    1103\n",
       "4063     465\n",
       "6336     717\n",
       "5480    1314\n",
       "3203     798\n",
       "8115    1416\n",
       "4500    1559\n",
       "8003    1771\n",
       "7023     906\n",
       "105     1488\n",
       "7652     826\n",
       "        ... \n",
       "6070     501\n",
       "7706    1167\n",
       "4856    1312\n",
       "7750    1060\n",
       "5828    1577\n",
       "578     1132\n",
       "7227    1276\n",
       "6584    1764\n",
       "2503    1359\n",
       "2266    1533\n",
       "3901    1584\n",
       "6602    1367\n",
       "7241    1652\n",
       "367     1421\n",
       "7997    1280\n",
       "2415     842\n",
       "8530     811\n",
       "8014    1102\n",
       "5294     938\n",
       "282      935\n",
       "2741     802\n",
       "2902    1238\n",
       "7661     974\n",
       "2460    1359\n",
       "586     1577\n",
       "5996     820\n",
       "1740    1326\n",
       "4595    1081\n",
       "6587    1334\n",
       "3011    1503\n",
       "Name: posts, Length: 5205, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_post_length(post):\n",
    "        \"\"\"Helper code to compute average word length of a post\"\"\"\n",
    "        return int(len(post.split()))\n",
    "\n",
    "testData = train_ie['posts']\n",
    "testData.apply(average_post_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AveragePostLengthExtractor():\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def average_post_length(self, post):\n",
    "        \"\"\"Helper code to compute average word length of a post\"\"\"\n",
    "        return round(len(post.split()))\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        print(type(df))\n",
    "        return (df.apply(self.average_post_length)).values.reshape(-1, 1)\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        print(\"fitting\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "test = \"I\\'m\"\n",
    "print(test in \"Hello I'm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first-person features: I, me, my, us, we, mine, our, I'm\n",
    "# two person: you, your, you're, you've\n",
    "# third person: he, him, she, her, they, them, their, his\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "class FirstPersonExtractor():\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.first_person_words = set(['I', 'me', 'my', 'us', 'we', 'mine', \"our\", \"I\\'m\"])\n",
    "\n",
    "    def count_first_person(self, post):\n",
    "        val = 0\n",
    "        for word in post:\n",
    "            if word in self.first_person_words:\n",
    "                val += 1\n",
    "        return val\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        return (df.apply(self.count_first_person)).values.reshape(-1, 1)\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "# first-person features: I, me, my, us, we, mine, our, I'm\n",
    "# two person: you, your, you're, you've\n",
    "# third person: he, him, she, her, they, them, their, his\n",
    "\n",
    "class SecondPersonExtractor():\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.second_person_words = set([\"you\", \"your\", \"you\\'re\", \"you\\'ve\"])\n",
    "\n",
    "    def count_second_person(self, post):\n",
    "        val = 0\n",
    "        for word in post:\n",
    "            if word in self.second_person_words:\n",
    "                val += 1\n",
    "        return val\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        return (df.apply(self.count_second_person)).values.reshape(-1, 1)\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class ThirdPersonExtractor():\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.third_person_words = set([\"he\", \"him\", \"she\", \"her\", \"they\", \"them\", \"their\", \"his\"])\n",
    "\n",
    "    def count_third_person(self, post):\n",
    "        val = 0\n",
    "        for word in post:\n",
    "            if word in self.third_person_words:\n",
    "                val += 1\n",
    "        return val\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        return (df.apply(self.count_third_person)).values.reshape(-1, 1)\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class count_vec():\n",
    "    def __init__(self):\n",
    "        self.count = CountVectorizer(ngram_range = (1, 2))\n",
    "    def transform(self, data):\n",
    "        print(\"yah\")\n",
    "        return self.count.transform(data)\n",
    "    def fit(self, data, labels):\n",
    "        self.count.fit(data)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgLengthDeviation():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def average_post_length(self, post):\n",
    "        \"\"\"Helper code to compute average word length of a post\"\"\"\n",
    "        return round(len(post.split()))\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        deviations = []\n",
    "        for posts in df:\n",
    "            avg = np.mean([len(post.split()) for post in posts.split(\"|||\") if post != \"\"])\n",
    "            deviations.append([avg - self.avgs[0], avg - self.avgs[1]])\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Calculates average post length for each class\"\"\"\n",
    "        labels = list(np.unique(y))\n",
    "        avgs = [0, 0]\n",
    "        labelCounts = [0, 0]\n",
    "        for (posts, userLabel) in zip(df, y):\n",
    "            avgs[labels.index(userLabel)] += np.mean([len(post.split()) for post in posts.split(\"|||\") if post != \"\"])\n",
    "            labelCounts[labels.index(userLabel)] += 1\n",
    "\n",
    "        avgs[0],avgs[1] = avgs[0] / labelCounts[0], avgs[1] / labelCounts[1]\n",
    "        self.avgs = avgs\n",
    "        self.labels = labels\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "pipeline = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('avg_len', AvgLengthDeviation()),\n",
    "        ('first_per', FirstPersonExtractor()),\n",
    "        ('second_per', SecondPersonExtractor()),\n",
    "        ('third_per', ThirdPersonExtractor()),\n",
    "        ('count_vec', count_vec())\n",
    "    ])),\n",
    "    ('clf', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yah\n",
      "yah\n",
      "IE accuracy 0.2403458213256484\n",
      "yah\n",
      "yah\n",
      "NS accuracy 0.12219020172910663\n",
      "yah\n",
      "yah\n",
      "FT accuracy 0.4634005763688761\n",
      "yah\n",
      "yah\n",
      "PJ accuracy 0.38962536023054756\n"
     ]
    }
   ],
   "source": [
    "# Train weight downweight factors\n",
    "downsize_factor_ie = sum(train_ie['IE'] == 'E')/len(train_ie['IE'])\n",
    "train_weights_ie = [1 if ie == 'E' else downsize_factor_ie for ie in train_ie['IE']]\n",
    "\n",
    "downsize_factor_ns = sum(train_ns['NS'] == 'S')/len(train_ns['NS'])\n",
    "train_weights_ns = [1 if ns == 'S' else downsize_factor_ns for ns in train_ns['NS']]\n",
    "\n",
    "downsize_factor_ft = sum(train_ft['FT'] == 'T')/len(train_ft['FT'])\n",
    "train_weights_ft = [1 if ft == 'T' else downsize_factor_ft for ft in train_ft['FT']]\n",
    "\n",
    "downsize_factor_pj = sum(train_pj['PJ'] == 'J')/len(train_pj['PJ'])\n",
    "train_weights_pj = [1 if pj == 'J' else downsize_factor_pj for pj in train_pj['PJ']]\n",
    "\n",
    "#IE dev set\n",
    "pipeline.fit(train_ie['posts'], train_ie['IE'], **{'clf__sample_weight': train_weights_ie})  \n",
    "predicted_ie = pipeline.predict(dev_ie['posts'])\n",
    "print(\"IE accuracy\", np.mean(predicted_ie == dev_ie['IE']))\n",
    "\n",
    "pipeline.fit(train_ns['posts'], train_ns['NS'], **{'clf__sample_weight': train_weights_ns})  \n",
    "predicted_ns = pipeline.predict(dev_ns['posts'])\n",
    "print(\"NS accuracy\", np.mean(predicted_ns == dev_ns['NS']))\n",
    "\n",
    "pipeline.fit(train_ft['posts'], train_ft['FT'], **{'clf__sample_weight': train_weights_ft})  \n",
    "predicted_ft = pipeline.predict(dev_ft['posts'])\n",
    "print(\"FT accuracy\", np.mean(predicted_ft == dev_ft['FT']))\n",
    "\n",
    "pipeline.fit(train_pj['posts'], train_pj['PJ'], **{'clf__sample_weight': train_weights_pj})  \n",
    "predicted_pj = pipeline.predict(dev_pj['posts'])\n",
    "print(\"PJ accuracy\", np.mean(predicted_pj == dev_pj['PJ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score IE [0.         0.38754647]\n",
      "f1 score NS [0.         0.21777093]\n",
      "f1 score FT [0.        0.6333202]\n",
      "f1 score PJ [0.         0.56076317]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BrandonLiu/anaconda3/envs/nlu/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1 score IE\", f1_score(dev_ie['IE'], predicted_ie, labels=['I', 'E'], average=None))\n",
    "\n",
    "print(\"f1 score NS\", f1_score(dev_ns['NS'], predicted_ns, labels=['N', 'S'], average=None))\n",
    "\n",
    "print(\"f1 score FT\", f1_score(dev_ft['FT'], predicted_ft, labels=['F', 'T'], average=None))\n",
    "\n",
    "print(\"f1 score PJ\", f1_score(dev_pj['PJ'], predicted_pj, labels=['P', 'J'], average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 1st, 2nd, 3rd person, countvec\n",
    "\n",
    "#IE accuracy: .247\n",
    "#NS accuracy: .346\n",
    "#FT accuracy: 0.494\n",
    "#PJ accuracy 0.390\n",
    "\n",
    "# f1 score IE [0.01950488 0.38839495]\n",
    "# f1 score NS [0.43072289 0.23274696]\n",
    "# f1 score FT [0.12723658 0.64366883]\n",
    "# f1 score PJ [0.         0.56076317]\n",
    "\n",
    "# bigrams: identical numbers\n",
    "#IE accuracy: .247\n",
    "#NS accuracy: .346\n",
    "#FT accuracy: 0.494\n",
    "#PJ accuracy 0.390\n",
    "\n",
    "\n",
    "# using avg post length deviations and count vec(no 1st, 2nd, 3rd): yah\n",
    "# yah\n",
    "# IE accuracy 0.2426512968299712\n",
    "# yah\n",
    "# yah\n",
    "# NS accuracy 0.22074927953890489\n",
    "# yah\n",
    "# yah\n",
    "# FT accuracy 0.47723342939481267\n",
    "# yah\n",
    "# yah\n",
    "# PJ accuracy 0.38962536023054756\n",
    "\n",
    "\n",
    "# f1 score IE [0.00605144 0.38826816]\n",
    "# f1 score NS [0.20935673 0.23181818]\n",
    "# f1 score FT [0.05026178 0.63936382]\n",
    "# f1 score PJ [0.         0.56076317]\n",
    "\n",
    "# using avg post length deviations, count vec, 1st, 2nd, 3rd with bigrams, unigrams\n",
    "#IE accuracy: .240\n",
    "#NS accuracy: .122\n",
    "#FT accuracy: .463\n",
    "#PJ accuracy: .390\n",
    "\n",
    "# f1 score IE [0.         0.38754647]\n",
    "# f1 score NS [0.         0.21777093]\n",
    "# f1 score FT [0.        0.6333202]\n",
    "# f1 score PJ [0.         0.56076317]\n",
    "\n",
    "# Prediction distribution\n",
    "# Counter({'E': 1735})\n",
    "# Counter({'S': 1735})\n",
    "# Counter({'T': 1735})\n",
    "# Counter({'J': 1735})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[['E']\n",
      " [1735]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(predicted_ie, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw counts\n",
      "I    6676\n",
      "E    1999\n",
      "Name: IE, dtype: int64\n",
      "N    7478\n",
      "S    1197\n",
      "Name: NS, dtype: int64\n",
      "F    4694\n",
      "T    3981\n",
      "Name: FT, dtype: int64\n",
      "P    5241\n",
      "J    3434\n",
      "Name: PJ, dtype: int64\n",
      "\n",
      "\n",
      "Prediction distribution\n",
      "Counter({'E': 1735})\n",
      "Counter({'S': 1735})\n",
      "Counter({'T': 1735})\n",
      "Counter({'J': 1735})\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw counts\")\n",
    "print(df_ie['IE'].value_counts())\n",
    "print(df_ns['NS'].value_counts())\n",
    "print(df_ft['FT'].value_counts())\n",
    "print(df_pj['PJ'].value_counts())\n",
    "\n",
    "\n",
    "import collections\n",
    "print(\"\\n\\nPrediction distribution\")\n",
    "print(collections.Counter(predicted_ie))\n",
    "print(collections.Counter(predicted_ns))\n",
    "print(collections.Counter(predicted_ft))\n",
    "print(collections.Counter(predicted_pj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
