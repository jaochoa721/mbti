{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "df = pd.read_csv('mbti_1.csv')\n",
    "\n",
    "# replace URLs\n",
    "\n",
    "# replace MBTI\n",
    "# https://stackoverflow.com/questions/16720541/python-string-replace-regular-expression/16720705\n",
    "mbti_pat = r\"ISFJ|ESFP|ISFP|ISTP|ENFP|ENFJ|INFJ|ESTP|ESFJ|ESTJ|ENTP|INFP|INTP|INTJ|ISTJ|ENTJ\"\n",
    "mbti_regex = re.compile(mbti_pat, re.IGNORECASE)\n",
    "MBTI_REP = '$MBTI$'\n",
    "\n",
    "# replace hashtags\n",
    "hashtag_pat = r\"(\\#[a-zA-Z0-9]+\\b)\"\n",
    "hashtag_regex = re.compile(hashtag_pat)\n",
    "HASHTAG_REP = '$HASHTAG$'\n",
    "\n",
    "# Replace links with $link$\n",
    "# https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url\n",
    "link_pat = r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
    "LINK_REP = '$LINK$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posts'] = df['posts'].apply(lambda x: re.sub(mbti_pat, MBTI_REP, x))\n",
    "df['posts'] = df['posts'].apply(lambda x: re.sub(hashtag_pat, HASHTAG_REP, x))\n",
    "df['posts'] = df['posts'].apply(lambda x: re.sub(link_pat, LINK_REP, x))\n",
    "\n",
    "\n",
    "df['posts'] = df['posts'].apply(lambda x: x.replace('|||', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IE'] = df['type'].apply(lambda x: 'I' if x[0] == 'I' else 'E')\n",
    "df['NS'] = df['type'].apply(lambda x: 'N' if x[1] == 'N' else 'S')\n",
    "df['FT'] = df['type'].apply(lambda x: 'F' if x[2] == 'F' else 'T')\n",
    "df['PJ'] = df['type'].apply(lambda x: 'P' if x[3] == 'P' else 'J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ie = df[['type', 'posts', 'IE']]\n",
    "df_ns = df[['type', 'posts', 'NS']]\n",
    "df_ft = df[['type', 'posts', 'FT']]\n",
    "df_pj = df[['type', 'posts', 'PJ']]\n",
    "\n",
    "train_pct = 0.6\n",
    "\n",
    "# indicates the location to split the data along\n",
    "# since dev/test are the same size\n",
    "test_split_position = 1.0 - (1.0 - train_pct) / 2\n",
    "test_split_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ie, dev_ie, test_ie = np.split(df_ie.sample(frac=1, random_state = 224), [int(train_pct*len(df_ie)), int(test_split_position*len(df_ie))])\n",
    "train_ns, dev_ns, test_ns = np.split(df_ns.sample(frac=1, random_state = 224), [int(train_pct*len(df_ns)), int(test_split_position*len(df_ns))])\n",
    "train_ft, dev_ft, test_ft = np.split(df_ft.sample(frac=1, random_state = 224), [int(train_pct*len(df_ft)), int(test_split_position*len(df_ft))])\n",
    "train_pj, dev_pj, test_pj = np.split(df_pj.sample(frac=1, random_state = 224), [int(train_pct*len(df_pj)), int(test_split_position*len(df_pj))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab dict, mapping word to unique number\n",
    "# keep track of vocab size (then +1 for UNK)\n",
    "def build_vocab_dict():\n",
    "    df = train_ie.copy(deep=True)\n",
    "    df.append(train_ns)\n",
    "    df.append(train_ft)\n",
    "    df.append(train_pj)\n",
    "    posts = df['posts']\n",
    "    counter = cv()\n",
    "    counter.fit(posts)\n",
    "    return counter.vocabulary_    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = build_vocab_dict()\n",
    "vocab_size = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95312\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "vocab_dict['UNK'] = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FILE_PATH = \"./glove.6B.50d.txt\"\n",
    "def loadWordVectors(tokens, filepath=DEFAULT_FILE_PATH, dimensions=50):\n",
    "    \"\"\"Read pretrained GloVe vectors\"\"\"\n",
    "    lines_so_far = 0\n",
    "    wordVectors = np.zeros((len(tokens), dimensions))\n",
    "    with open(filepath, 'rb') as ifs:\n",
    "        for line in ifs:\n",
    "#             if lines_so_far % 1000 == 0:\n",
    "#                 print(\"======> passed line \" + str(lines_so_far))\n",
    "            lines_so_far += 1\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            row = line.split()\n",
    "            token = row[0]\n",
    "            if token not in tokens:\n",
    "                continue\n",
    "            data = [float(x) for x in row[1:]]\n",
    "            if len(data) != dimensions:\n",
    "                raise RuntimeError(\"wrong number of dimensions\")\n",
    "            wordVectors[tokens[token]] = np.asarray(data)\n",
    "    return wordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have matrix that's [vocab_size, embedding size] \n",
    "glove_dimensions = 50\n",
    "def build_embeddings_matrix():\n",
    "    embed_matrix = loadWordVectors(vocab_dict)\n",
    "    \n",
    "    for word in vocab_dict:\n",
    "        embed_matrix_word_index = vocab_dict[word]\n",
    "        if len(embed_matrix[embed_matrix_word_index]) < glove_dimensions:\n",
    "            embed_matrix[embed_matrix_word_index] = [float(0) for x in range(glove_dimensions)]\n",
    "            vocab_dict[word] = vocab_dict['UNK']\n",
    "    return embed_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrix = build_embeddings_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def build_data_matrices(data, type_that_should_be_one):\n",
    "    temp = [[vocab_dict[word] if word in vocab_dict else vocab_dict['UNK'] for word in post.split()]\\\n",
    "                  for post in data['posts'] ]\n",
    "    data_posts = np.array(list(itertools.zip_longest(*temp, fillvalue=vocab_size))).T\n",
    "#     data_labels = np.zeros((len(data['type']) , 2))\n",
    "    data_labels = np.asarray([ 1 if type == type_that_should_be_one else 0 for type in data['type']])\n",
    "    return data_posts, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ie_data, train_ie_labels = build_data_matrices(train_ie, 'E')\n",
    "train_ns_data, train_ns_labels = build_data_matrices(train_ns, 'N')\n",
    "train_ft_data, train_ft_labels = build_data_matrices(train_ft, 'F')\n",
    "train_pj_data, train_pj_labels = build_data_matrices(train_pj, 'P')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5205,)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ie_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_post_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data_matrix, data_labels, hidden_size=256, lr=0.005):\n",
    "    n_features = glove_dimensions\n",
    "    n_classes = 2\n",
    "    max_grad_norm = 5.\n",
    "\n",
    "    # add placeholders\n",
    "    input_placeholder = tf.placeholder(tf.int32, shape=(None, max_post_len))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(None, ))\n",
    "\n",
    "    # add embedding layer!\n",
    "    x = tf.nn.embedding_lookup(embed_matrix, input_placeholder)\n",
    "    # x = tf.nn.dropout(x, 0.8)\n",
    "\n",
    "    # build model\n",
    "    U = tf.get_variable(\"U\", shape=[hidden_size, n_classes], dtype=tf.float64, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable(\"b\", shape=[1, n_classes], dtype=tf.float64, initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "    rnn_cell = tf.nn.rnn_cell.DropoutWrapper(rnn_cell, output_keep_prob=0.8)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(rnn_cell, x, dtype=tf.float64)\n",
    "    # print \"batch size \", final_state[1].shape[1]\n",
    "\n",
    "    h = final_state[1]\n",
    "    # h = tf.get_variable(\"final_state\", dtype=tf.float64, initializer=h, trainable=False)\n",
    "    # h = tf.assign_add(h, final_state[1])\n",
    "    pred = tf.matmul(h, U) + b\n",
    "\n",
    "    labels_one_hot = tf.one_hot(labels_placeholder, n_classes)\n",
    "    loss_op = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=pred)\n",
    "    loss_op = tf.reduce_mean(loss_op, 0)\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss_op, params)\n",
    "    clippied_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.apply_gradients(zip(clippied_gradients, params))\n",
    "    # train_op = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss_op)\n",
    "    return pred, input_placeholder, labels_placeholder, train_op, loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(data_matrix, data_labels, batch_size, max_sequence_length):\n",
    "    batch_list = []\n",
    "    indices = []\n",
    "    n_matrix_rows = data_matrix.shape[0] \n",
    "    for i in range(0, n_matrix_rows, batch_size):\n",
    "        batch = data_matrix[i : i+batch_size, : max_sequence_length]\n",
    "        batch_label = data_labels[i : i+batch_size]\n",
    "        batch_list.append((batch, batch_label))\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def get_accuracy(pred, labels, classes):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    prec = 2\n",
    "    accuracy = accuracy_score(labels, pred)\n",
    "    print (\"Accuracy: \" + str(round(accuracy * 100, prec)) + \"%\")\n",
    "    micro_f1 = f1_score(labels, pred, average=\"micro\")\n",
    "    macro_f1 = f1_score(labels, pred, average=\"macro\")\n",
    "    class_f1 = f1_score(labels, pred, average=None)\n",
    "    print (\"Micro F1 score: \" + str(round(micro_f1 * 100, prec)) + \"%\")\n",
    "    print (\"Macro F1 score: \" + str(round(macro_f1 * 100, prec)) + \"%\")\n",
    "    for class_name, score in zip(classes, class_f1):\n",
    "        print(\"F1 score for \" + class_name + \": \", str(round(score*100, 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_matrix, data_labels, save_path, hidden_size=256, lr=0.005, saved_model_path=None, RESUME=False, batch_size=256, n_epochs=30):\n",
    "    tf.reset_default_graph()\n",
    "    _, input_placeholder, labels_placeholder, train_op, loss_op = build_model(data_matrix, data_labels, hidden_size=hidden_size, lr=lr)\t\n",
    "    saver = tf.train.Saver()\n",
    "    avg_loss_list = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if RESUME:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, saved_model_path)\n",
    "            print(\"Model restored.\")\n",
    "            \n",
    "        minibatches = get_minibatches(data_matrix, data_labels, batch_size, max_post_len)\n",
    "        for i in range(n_epochs):\n",
    "            batch_loss_list = []\n",
    "            print (\"Epoch \" + str(i+1) + \": \")\n",
    "            for tup in minibatches:\n",
    "                _, loss = sess.run([train_op, loss_op], feed_dict={input_placeholder: tup[0], labels_placeholder: tup[1]})\n",
    "                batch_loss_list.append(loss)\n",
    "            avg_loss_list.append(np.mean(batch_loss_list))\n",
    "            print (\"=====>loss: \" + str(avg_loss_list[i]) + \" \")\n",
    "            if (i > 0) and (avg_loss_list[i] < avg_loss_list[i-1]):\n",
    "                tmp_path = save_path + \"--smallest loss\"\n",
    "                saver.save(sess, tmp_path)\n",
    "                print (\"New min loss at epoch %s! Model saved in path %s\" % (str(i+1), tmp_path))\n",
    "        saver.save(sess, save_path)\n",
    "        print(\"Final model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_matrix, data_labels, saved_model_path, classes, batch_size=256):\n",
    "    tf.reset_default_graph()\n",
    "    pred, input_placeholder, labels_placeholder, _, loss_op = build_model(data_matrix, data_labels)\n",
    "    saver = tf.train.Saver()\n",
    "    loss_list = []\n",
    "    label_list= []\n",
    "    pred_list = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, saved_model_path)\n",
    "        print(\"Model restored.\")\n",
    "\n",
    "        minibatches = get_minibatches(data_matrix, data_labels, batch_size, max_post_len)\n",
    "        for tup in minibatches:\n",
    "            pred_temp, loss, labels_temp = sess.run([pred, loss_op, labels_placeholder], feed_dict={input_placeholder: tup[0], labels_placeholder: tup[1]})\n",
    "            for i, row in enumerate(pred_temp):\n",
    "                pred_list.append(np.where(row == max(row))[0][0])\n",
    "\n",
    "            loss_list.append(loss)\n",
    "\n",
    "        print (\"Loss: \" + str(np.mean(loss_list)) + \"\\n\")\n",
    "\n",
    "    get_accuracy(pred_list, data_labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "=====>loss: 0.06270280733510868 \n",
      "Epoch 2: \n",
      "=====>loss: 4.3880243354232285e-15 \n",
      "New min loss at epoch 2! Model saved in path ./models--smallest loss\n",
      "Final model saved in path: ./models\n"
     ]
    }
   ],
   "source": [
    "train(train_ie_data, train_ie_labels, \"./models\", n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ie_data, dev_ie_labels = build_data_matrices(dev_ie, 'E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models\n",
      "Model restored.\n",
      "Loss: 2.2204460492503127e-15\n",
      "\n",
      "Accuracy: 100.0%\n",
      "Micro F1 score: 100.0%\n",
      "Macro F1 score: 100.0%\n",
      "F1 score for E:  100.0%\n"
     ]
    }
   ],
   "source": [
    "test(dev_ie_data, dev_ie_labels, \"./models\", ['E', 'I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
